Sender: LSF System <lsfadmin@lo-a2-071>
Subject: Job 5241972: <python optionals/optional2/optional2.py --model LSTM> in cluster <leonhard> Done

Job <python optionals/optional2/optional2.py --model LSTM> was submitted from host <lo-login-01> by user <bberabi> in cluster <leonhard> at Tue Mar 17 03:35:01 2020
Job was executed on host(s) <2*lo-a2-071>, in queue <normal.24h>, as user <bberabi> in cluster <leonhard> at Tue Mar 17 03:35:06 2020
</cluster/home/bberabi> was used as the home directory.
</cluster/home/bberabi/mlfhc> was used as the working directory.
Started at Tue Mar 17 03:35:06 2020
Terminated at Tue Mar 17 05:08:43 2020
Results reported at Tue Mar 17 05:08:43 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python optionals/optional2/optional2.py --model LSTM
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   18996.17 sec.
    Max Memory :                                 1491 MB
    Average Memory :                             1399.41 MB
    Total Requested Memory :                     4000.00 MB
    Delta Memory :                               2509.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                20
    Run time :                                   5633 sec.
    Turnaround time :                            5622 sec.

The output (if any) follows:

2020-03-17 03:35:15.359966: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA
2020-03-17 03:35:15.370747: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz
2020-03-17 03:35:15.371107: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x52220b0 executing computations on platform Host. Devices:
2020-03-17 03:35:15.371142: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Host, Default Version
Loading Model : /cluster/home/bberabi/mlfhc/models/lstm/best_model_lstm_mitbih_976.h5
Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
lstm_1 (LSTM)                (None, 187, 64)           16896     
_________________________________________________________________
lstm_2 (LSTM)                (None, 187, 64)           33024     
_________________________________________________________________
lstm_3 (LSTM)                (None, 64)                33024     
_________________________________________________________________
dense_1 (Dense)              (None, 64)                4160      
_________________________________________________________________
dense_2 (Dense)              (None, 64)                4160      
_________________________________________________________________
dense_3 (Dense)              (None, 5)                 325       
=================================================================
Total params: 91,589
Trainable params: 91,589
Non-trainable params: 0
_________________________________________________________________
Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
lstm_1 (LSTM)                (None, 187, 64)           16896     
_________________________________________________________________
lstm_2 (LSTM)                (None, 187, 64)           33024     
_________________________________________________________________
lstm_3 (LSTM)                (None, 64)                33024     
=================================================================
Total params: 82,944
Trainable params: 82,944
Non-trainable params: 0
_________________________________________________________________
Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
lstm_1 (LSTM)                (None, 187, 64)           16896     
_________________________________________________________________
lstm_2 (LSTM)                (None, 187, 64)           33024     
_________________________________________________________________
lstm_3 (LSTM)                (None, 64)                33024     
_________________________________________________________________
dense_1 (Dense)              (None, 64)                4160      
_________________________________________________________________
batch_normalization_1 (Batch (None, 64)                256       
_________________________________________________________________
dense_2 (Dense)              (None, 64)                4160      
_________________________________________________________________
batch_normalization_2 (Batch (None, 64)                256       
_________________________________________________________________
dense_3 (Dense)              (None, 64)                4160      
_________________________________________________________________
batch_normalization_3 (Batch (None, 64)                256       
_________________________________________________________________
dense_4 (Dense)              (None, 64)                4160      
_________________________________________________________________
batch_normalization_4 (Batch (None, 64)                256       
_________________________________________________________________
dense_5 (Dense)              (None, 1)                 65        
=================================================================
Total params: 100,673
Trainable params: 100,161
Non-trainable params: 512
_________________________________________________________________
Best model will be saved in  optional2_LSTM.h5
Train on 10476 samples, validate on 1165 samples
Epoch 1/200
 - 93s - loss: 0.5641 - acc: 0.7060 - val_loss: 0.5712 - val_acc: 0.6996

Epoch 00001: val_acc improved from -inf to 0.69957, saving model to optional2_LSTM.h5
Epoch 2/200
 - 87s - loss: 0.3781 - acc: 0.8315 - val_loss: 0.4114 - val_acc: 0.8240

Epoch 00002: val_acc improved from 0.69957 to 0.82403, saving model to optional2_LSTM.h5
Epoch 3/200
 - 85s - loss: 0.2714 - acc: 0.8929 - val_loss: 0.3160 - val_acc: 0.8464

Epoch 00003: val_acc improved from 0.82403 to 0.84635, saving model to optional2_LSTM.h5
Epoch 4/200
 - 85s - loss: 0.2318 - acc: 0.9088 - val_loss: 0.2819 - val_acc: 0.8815

Epoch 00004: val_acc improved from 0.84635 to 0.88155, saving model to optional2_LSTM.h5
Epoch 5/200
 - 85s - loss: 0.3687 - acc: 0.8318 - val_loss: 0.4288 - val_acc: 0.7863

Epoch 00005: val_acc did not improve from 0.88155
Epoch 6/200
 - 85s - loss: 0.2633 - acc: 0.8929 - val_loss: 0.3421 - val_acc: 0.8232

Epoch 00006: val_acc did not improve from 0.88155
Epoch 7/200
 - 85s - loss: 0.2103 - acc: 0.9190 - val_loss: 0.3815 - val_acc: 0.8609

Epoch 00007: val_acc did not improve from 0.88155
Epoch 8/200
 - 85s - loss: 0.1969 - acc: 0.9242 - val_loss: 0.2868 - val_acc: 0.8687

Epoch 00008: val_acc did not improve from 0.88155
Epoch 9/200
 - 85s - loss: 0.1812 - acc: 0.9304 - val_loss: 0.2554 - val_acc: 0.9090

Epoch 00009: val_acc improved from 0.88155 to 0.90901, saving model to optional2_LSTM.h5
Epoch 10/200
 - 85s - loss: 0.1745 - acc: 0.9350 - val_loss: 0.3288 - val_acc: 0.8549

Epoch 00010: val_acc did not improve from 0.90901
Epoch 11/200
 - 84s - loss: 0.1603 - acc: 0.9394 - val_loss: 0.1831 - val_acc: 0.9305

Epoch 00011: val_acc improved from 0.90901 to 0.93047, saving model to optional2_LSTM.h5
Epoch 12/200
 - 85s - loss: 0.1590 - acc: 0.9387 - val_loss: 0.2521 - val_acc: 0.9056

Epoch 00012: val_acc did not improve from 0.93047
Epoch 13/200
 - 85s - loss: 0.1526 - acc: 0.9437 - val_loss: 0.3224 - val_acc: 0.8764

Epoch 00013: val_acc did not improve from 0.93047
Epoch 14/200
 - 85s - loss: 0.1447 - acc: 0.9474 - val_loss: 0.2235 - val_acc: 0.8987

Epoch 00014: val_acc did not improve from 0.93047
Epoch 15/200
 - 85s - loss: 0.1282 - acc: 0.9535 - val_loss: 0.1187 - val_acc: 0.9614

Epoch 00015: val_acc improved from 0.93047 to 0.96137, saving model to optional2_LSTM.h5
Epoch 16/200
 - 85s - loss: 0.1233 - acc: 0.9534 - val_loss: 0.2337 - val_acc: 0.9124

Epoch 00016: val_acc did not improve from 0.96137
Epoch 17/200
 - 85s - loss: 0.1208 - acc: 0.9551 - val_loss: 0.2115 - val_acc: 0.9142

Epoch 00017: val_acc did not improve from 0.96137
Epoch 18/200
 - 85s - loss: 0.1157 - acc: 0.9567 - val_loss: 0.1075 - val_acc: 0.9639

Epoch 00018: val_acc improved from 0.96137 to 0.96395, saving model to optional2_LSTM.h5
Epoch 19/200
 - 84s - loss: 0.1074 - acc: 0.9591 - val_loss: 0.1584 - val_acc: 0.9391

Epoch 00019: val_acc did not improve from 0.96395
Epoch 20/200
 - 84s - loss: 0.1083 - acc: 0.9603 - val_loss: 0.1444 - val_acc: 0.9399

Epoch 00020: val_acc did not improve from 0.96395
Epoch 21/200
 - 85s - loss: 0.1114 - acc: 0.9611 - val_loss: 0.1453 - val_acc: 0.9391

Epoch 00021: val_acc did not improve from 0.96395
Epoch 22/200
 - 84s - loss: 0.1002 - acc: 0.9639 - val_loss: 0.0995 - val_acc: 0.9665

Epoch 00022: val_acc improved from 0.96395 to 0.96652, saving model to optional2_LSTM.h5
Epoch 23/200
 - 83s - loss: 0.0891 - acc: 0.9670 - val_loss: 0.1694 - val_acc: 0.9408

Epoch 00023: val_acc did not improve from 0.96652
Epoch 24/200
 - 83s - loss: 0.0893 - acc: 0.9673 - val_loss: 0.1327 - val_acc: 0.9468

Epoch 00024: val_acc did not improve from 0.96652
Epoch 25/200
 - 83s - loss: 0.0933 - acc: 0.9666 - val_loss: 0.1528 - val_acc: 0.9365

Epoch 00025: val_acc did not improve from 0.96652
Epoch 26/200
 - 83s - loss: 0.0893 - acc: 0.9676 - val_loss: 0.1307 - val_acc: 0.9476

Epoch 00026: val_acc did not improve from 0.96652
Epoch 27/200
 - 83s - loss: 0.0809 - acc: 0.9719 - val_loss: 0.1244 - val_acc: 0.9614

Epoch 00027: val_acc did not improve from 0.96652
Epoch 28/200
 - 83s - loss: 0.0814 - acc: 0.9711 - val_loss: 0.1335 - val_acc: 0.9622

Epoch 00028: val_acc did not improve from 0.96652
Epoch 29/200
 - 83s - loss: 0.0771 - acc: 0.9720 - val_loss: 0.1067 - val_acc: 0.9614

Epoch 00029: val_acc did not improve from 0.96652

Epoch 00029: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.
Epoch 30/200
 - 83s - loss: 0.0630 - acc: 0.9792 - val_loss: 0.0642 - val_acc: 0.9845

Epoch 00030: val_acc improved from 0.96652 to 0.98455, saving model to optional2_LSTM.h5
Epoch 31/200
 - 83s - loss: 0.0448 - acc: 0.9860 - val_loss: 0.1024 - val_acc: 0.9665

Epoch 00031: val_acc did not improve from 0.98455
Epoch 32/200
 - 84s - loss: 0.0451 - acc: 0.9848 - val_loss: 0.0614 - val_acc: 0.9854

Epoch 00032: val_acc improved from 0.98455 to 0.98541, saving model to optional2_LSTM.h5
Epoch 33/200
 - 84s - loss: 0.0496 - acc: 0.9843 - val_loss: 0.0953 - val_acc: 0.9751

Epoch 00033: val_acc did not improve from 0.98541
Epoch 34/200
 - 84s - loss: 0.0483 - acc: 0.9841 - val_loss: 0.0706 - val_acc: 0.9760

Epoch 00034: val_acc did not improve from 0.98541
Epoch 35/200
 - 84s - loss: 0.0580 - acc: 0.9799 - val_loss: 0.0647 - val_acc: 0.9777

Epoch 00035: val_acc did not improve from 0.98541
Epoch 36/200
 - 85s - loss: 0.0499 - acc: 0.9847 - val_loss: 0.0684 - val_acc: 0.9811

Epoch 00036: val_acc did not improve from 0.98541
Epoch 37/200
 - 85s - loss: 0.0394 - acc: 0.9871 - val_loss: 0.0502 - val_acc: 0.9863

Epoch 00037: val_acc improved from 0.98541 to 0.98627, saving model to optional2_LSTM.h5
Epoch 38/200
 - 85s - loss: 0.0435 - acc: 0.9860 - val_loss: 0.0524 - val_acc: 0.9880

Epoch 00038: val_acc improved from 0.98627 to 0.98798, saving model to optional2_LSTM.h5
Epoch 39/200
 - 84s - loss: 0.0399 - acc: 0.9863 - val_loss: 0.0582 - val_acc: 0.9811

Epoch 00039: val_acc did not improve from 0.98798
Epoch 40/200
 - 84s - loss: 0.0364 - acc: 0.9882 - val_loss: 0.0651 - val_acc: 0.9777

Epoch 00040: val_acc did not improve from 0.98798
Epoch 41/200
 - 84s - loss: 0.0425 - acc: 0.9857 - val_loss: 0.0817 - val_acc: 0.9751

Epoch 00041: val_acc did not improve from 0.98798
Epoch 42/200
 - 83s - loss: 0.0404 - acc: 0.9869 - val_loss: 0.0577 - val_acc: 0.9820

Epoch 00042: val_acc did not improve from 0.98798
Epoch 43/200
 - 84s - loss: 0.0368 - acc: 0.9888 - val_loss: 0.1833 - val_acc: 0.9391

Epoch 00043: val_acc did not improve from 0.98798
Epoch 44/200
 - 84s - loss: 0.0412 - acc: 0.9865 - val_loss: 0.0613 - val_acc: 0.9777

Epoch 00044: val_acc did not improve from 0.98798
Epoch 45/200
 - 83s - loss: 0.0373 - acc: 0.9870 - val_loss: 0.0982 - val_acc: 0.9742

Epoch 00045: val_acc did not improve from 0.98798

Epoch 00045: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.
Epoch 46/200
 - 84s - loss: 0.0260 - acc: 0.9926 - val_loss: 0.0694 - val_acc: 0.9820

Epoch 00046: val_acc did not improve from 0.98798
Epoch 47/200
 - 83s - loss: 0.0232 - acc: 0.9932 - val_loss: 0.0437 - val_acc: 0.9880

Epoch 00047: val_acc did not improve from 0.98798
Epoch 48/200
 - 84s - loss: 0.0200 - acc: 0.9945 - val_loss: 0.0437 - val_acc: 0.9888

Epoch 00048: val_acc improved from 0.98798 to 0.98884, saving model to optional2_LSTM.h5
Epoch 49/200
 - 83s - loss: 0.0218 - acc: 0.9943 - val_loss: 0.0557 - val_acc: 0.9820

Epoch 00049: val_acc did not improve from 0.98884
Epoch 50/200
 - 84s - loss: 0.0216 - acc: 0.9933 - val_loss: 0.0534 - val_acc: 0.9871

Epoch 00050: val_acc did not improve from 0.98884
Epoch 51/200
 - 83s - loss: 0.0223 - acc: 0.9941 - val_loss: 0.0620 - val_acc: 0.9803

Epoch 00051: val_acc did not improve from 0.98884
Epoch 52/200
 - 83s - loss: 0.0200 - acc: 0.9939 - val_loss: 0.0399 - val_acc: 0.9897

Epoch 00052: val_acc improved from 0.98884 to 0.98970, saving model to optional2_LSTM.h5
Epoch 53/200
 - 83s - loss: 0.0174 - acc: 0.9950 - val_loss: 0.0482 - val_acc: 0.9871

Epoch 00053: val_acc did not improve from 0.98970
Epoch 54/200
 - 83s - loss: 0.0196 - acc: 0.9940 - val_loss: 0.0482 - val_acc: 0.9880

Epoch 00054: val_acc did not improve from 0.98970
Epoch 55/200
 - 83s - loss: 0.0211 - acc: 0.9941 - val_loss: 0.0383 - val_acc: 0.9871

Epoch 00055: val_acc did not improve from 0.98970
Epoch 56/200
 - 85s - loss: 0.0172 - acc: 0.9948 - val_loss: 0.0441 - val_acc: 0.9914

Epoch 00056: val_acc improved from 0.98970 to 0.99142, saving model to optional2_LSTM.h5
Epoch 57/200
 - 86s - loss: 0.0172 - acc: 0.9946 - val_loss: 0.0673 - val_acc: 0.9820

Epoch 00057: val_acc did not improve from 0.99142
Epoch 58/200
 - 85s - loss: 0.0178 - acc: 0.9945 - val_loss: 0.0386 - val_acc: 0.9871

Epoch 00058: val_acc did not improve from 0.99142
Epoch 59/200
 - 85s - loss: 0.0247 - acc: 0.9927 - val_loss: 0.0472 - val_acc: 0.9828

Epoch 00059: val_acc did not improve from 0.99142
Epoch 60/200
 - 85s - loss: 0.0157 - acc: 0.9952 - val_loss: 0.0562 - val_acc: 0.9845

Epoch 00060: val_acc did not improve from 0.99142
Epoch 61/200
 - 85s - loss: 0.0171 - acc: 0.9951 - val_loss: 0.0635 - val_acc: 0.9777

Epoch 00061: val_acc did not improve from 0.99142
Epoch 62/200
 - 84s - loss: 0.0167 - acc: 0.9958 - val_loss: 0.0386 - val_acc: 0.9880

Epoch 00062: val_acc did not improve from 0.99142
Epoch 63/200
 - 85s - loss: 0.0158 - acc: 0.9957 - val_loss: 0.0445 - val_acc: 0.9871

Epoch 00063: val_acc did not improve from 0.99142

Epoch 00063: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.
Epoch 64/200
 - 84s - loss: 0.0131 - acc: 0.9961 - val_loss: 0.0441 - val_acc: 0.9906

Epoch 00064: val_acc did not improve from 0.99142
Epoch 65/200
 - 84s - loss: 0.0097 - acc: 0.9975 - val_loss: 0.0364 - val_acc: 0.9897

Epoch 00065: val_acc did not improve from 0.99142
Epoch 66/200
 - 83s - loss: 0.0093 - acc: 0.9979 - val_loss: 0.0513 - val_acc: 0.9897
Using TensorFlow backend.

Epoch 00066: val_acc did not improve from 0.99142
Epoch 00066: early stopping
Test accuracy score : 0.9917554105118516 
